{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\") # Theme for plots as Dark\n",
    "sns.set_palette(\"viridis\")\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.callback import EarlyStopping\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_validate, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, mean_squared_log_error\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor, HistGradientBoostingRegressor, IsolationForest\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import optuna\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from catboost import Pool, CatBoostRegressor, cv\n",
    "import sys\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\", index_col=\"ID\")\n",
    "test_data = pd.read_csv(\"test.csv\", index_col=\"ID\")\n",
    "sub = pd.read_csv(\"sample_submission.csv\", index_col=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(6)\n",
    "\n",
    "X = train_data.drop([\"y\"],axis=1)\n",
    "y = train_data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmmodel = LGBMRegressor(random_state=seed, verbose=-1)\n",
    "xgbmodel = XGBRegressor(random_state=seed)\n",
    "catmodel = CatBoostRegressor(random_state=seed, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CV RMSLE score of LGBM is \",np.sqrt(-cross_val_score(lgbmmodel,X,y,cv=3, scoring = 'neg_mean_squared_log_error').mean()))\n",
    "print(\"CV RMSLE score of XGB is \",np.sqrt(-cross_val_score(xgbmodel,X,y,cv=3, scoring = 'neg_mean_squared_log_error').mean()))\n",
    "print(\"CV RMSLE score of CAT is \",np.sqrt(-cross_val_score(catmodel,X,y,cv=3, scoring = 'neg_mean_squared_log_error').mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generating Fold 1\n",
      "  Initial Size = 40118 | Final Size = 41374\n",
      "\n",
      "> Generating Fold 2\n",
      "  Initial Size = 41374 | Final Size = 41868\n",
      "\n",
      "> Generating Fold 3\n",
      "  Initial Size = 41868 | Final Size = 42179\n",
      "\n",
      "> Generating Fold 4\n",
      "  Initial Size = 42179 | Final Size = 42401\n",
      "\n",
      "> Generating Fold 5\n",
      "  Initial Size = 42401 | Final Size = 42551\n",
      "\n",
      "> Generating Fold 6\n",
      "  Initial Size = 42551 | Final Size = 42661\n",
      "\n",
      "> Generating Fold 7\n",
      "  Initial Size = 42661 | Final Size = 42749\n",
      "\n",
      "> Generating Fold 8\n",
      "  Initial Size = 42749 | Final Size = 42839\n",
      "\n",
      "> Generating Fold 9\n",
      "  Initial Size = 42839 | Final Size = 42917\n",
      "\n",
      "> Generating Fold 10\n",
      "  Initial Size = 42917 | Final Size = 42998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = test_data.columns\n",
    "\n",
    "for fold in range(10):\n",
    "    print(f\"> Generating Fold {fold+1}\")\n",
    "    print(f\"  Initial Size = {train_data.shape[0]}\",end=\" | \")\n",
    "    \n",
    "    lgbmmodel.fit(X,y)\n",
    "    xgbmodel.fit(X,y)\n",
    "    catmodel.fit(X,y)\n",
    "    \n",
    "    extra_train = test_data.copy()\n",
    "    extra_train[\"LGBM\"] = lgbmmodel.predict(extra_train[cols])\n",
    "    extra_train[\"XGB\"] = xgbmodel.predict(extra_train[cols])\n",
    "    extra_train[\"CAT\"] = catmodel.predict(extra_train[cols])\n",
    "    extra_train[\"STD\"] = np.std(extra_train[[\"LGBM\",\"XGB\",\"CAT\"]],axis=1)\n",
    "    extra_train[\"MEAN\"] = np.mean(extra_train[[\"LGBM\",\"XGB\",\"CAT\"]],axis=1)\n",
    "\n",
    "    STD_THRESHOLD = extra_train[\"STD\"].quantile(0.5)\n",
    "    extra_train = extra_train[extra_train[\"STD\"]<=STD_THRESHOLD]\n",
    "    \n",
    "    MEAN_THRESHOLD = 0.25\n",
    "    extra_train = pd.concat([extra_train[extra_train[\"MEAN\"]%1<MEAN_THRESHOLD],extra_train[extra_train[\"MEAN\"]%1>(1-MEAN_THRESHOLD)]])\n",
    "    extra_train[\"y\"] = np.round(extra_train[\"MEAN\"])\n",
    "\n",
    "    train_data = pd.concat([train_data,extra_train[train_data.columns]])\n",
    "    train_data.drop_duplicates(inplace=True)\n",
    "    train_data.reset_index(inplace=True,drop=True)\n",
    "    print(f\"Final Size = {train_data.shape[0]}\\n\")\n",
    "\n",
    "    X = train_data.drop([\"y\"],axis=1)\n",
    "    y = train_data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac=1.0)\n",
    "X = train_data.drop([\"y\"],axis=1)\n",
    "y = train_data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New CV RMSLE score of LGBM is \",np.sqrt(-cross_val_score(lgbmmodel,X,y,cv=3, scoring = 'neg_mean_squared_log_error').mean()))\n",
    "print(\"New CV RMSLE score of XGB is \",np.sqrt(-cross_val_score(xgbmodel,X,y,cv=3, scoring = 'neg_mean_squared_log_error').mean()))\n",
    "print(\"New CV RMSLE score of CAT is \",np.sqrt(-cross_val_score(catmodel,X,y,cv=3, scoring = 'neg_mean_squared_log_error').mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import numpy as np\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lgbm_params = {\n",
    "        \"random_state\": seed,\n",
    "        'n_estimators': 5000,        \n",
    "        \"max_depth\": trial.suggest_int('max_depth', 2, 11),\n",
    "        \"learning_rate\": trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        \"min_child_weight\": trial.suggest_float('min_child_weight', 0.5, 4),\n",
    "        \"min_child_samples\": trial.suggest_int('min_child_samples', 1, 250),\n",
    "        \"subsample\": trial.suggest_float('subsample', 0.2, 1),\n",
    "        \"subsample_freq\": trial.suggest_int('subsample_freq', 0, 5),\n",
    "        \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.2, 1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 64),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        \"metric\": \"rmse\",  # Changed to always use RMSE\n",
    "        \"boosting_type\": \"gbdt\",    \n",
    "        \"objective\": 'regression',\n",
    "        \"device\": \"cpu\",\n",
    "        \"verbose\": -1,\n",
    "        \"early_stopping_rounds\": 25,\n",
    "    }\n",
    "    \n",
    "    cv = RepeatedKFold(n_splits=4, n_repeats=1, random_state=seed)\n",
    "    scores = []\n",
    "    \n",
    "    for i, (tr, val) in tqdm(enumerate(cv.split(X, y)), total=4):\n",
    "        X_train, X_test = X.iloc[tr, :], X.iloc[val, :]\n",
    "        y_train, y_test = y.iloc[tr], y.iloc[val]\n",
    "\n",
    "        lgbmmodel = LGBMRegressor(**lgbm_params)\n",
    "        lgbmmodel.fit(\n",
    "            X_train, y_train, \n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric='rmse',\n",
    "        )\n",
    "        \n",
    "        y_pred = lgbmmodel.predict(X_test)\n",
    "        msle = mean_squared_log_error(y_test, y_pred)\n",
    "        rmsle = np.sqrt(msle)\n",
    "        scores.append(rmsle)\n",
    "    \n",
    "    mean_rmsle = np.mean(scores)\n",
    "    print(f\" > Mean RMSLE of LGBM = {mean_rmsle:.4f}\", file=sys.stderr)\n",
    "    return mean_rmsle\n",
    "\n",
    "# Create and run the study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, timeout=5000)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best RMSLE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: {'max_depth': 14, 'learning_rate': 0.0016646618260675332, 'min_child_weight': 2.3502628015430296, 'min_child_samples': 86, 'subsample': 0.6188009687544263, 'subsample_freq': 1, 'colsample_bytree': 0.36345733100814603, 'num_leaves': 34, 'lambda_l1': 2.407605586973264e-05, 'lambda_l2': 0.005280242664440079}\n",
    "Best RMSLE: 0.020562794021959464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: {'max_depth': 8, 'learning_rate': 0.08510271028056993, 'min_child_weight': 3.770439629993889, 'min_child_samples': 84, 'subsample': 0.7900530607404135, 'subsample_freq': 3, 'colsample_bytree': 0.4825271131189338, 'num_leaves': 11, 'lambda_l1': 5.3464140299853975, 'lambda_l2': 3.891889594418509e-06}\n",
    "Best RMSLE: 0.02055562526899524"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: {'max_depth': 4, 'learning_rate': 0.003853343265751353, 'min_child_weight': 1.8238667242311086, 'min_child_samples': 88, 'subsample': 0.20172390039981303, 'subsample_freq': 3, 'colsample_bytree': 0.4391384113015727, 'num_leaves': 21, 'lambda_l1': 0.011023766128262906, 'lambda_l2': 0.6331521226723741}\n",
    "Best RMSLE: 0.020565202328586323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    'n_estimators' : 8000,  \n",
    "    \"random_state\": seed,\n",
    "    \"boosting_type\": \"gbdt\",    \n",
    "    \"objective\":'regression',\n",
    "    \"device\": \"cpu\",\n",
    "    \"verbose\": -1,\n",
    "    \"early_stopping_rounds\" : 100,\n",
    "    'max_depth': 14,\n",
    "    'learning_rate': 0.0016646618260675332,\n",
    "    'min_child_weight': 2.3502628015430296,\n",
    "    'min_child_samples': 86,\n",
    "    'subsample': 0.6188009687544263,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.36345733100814603,\n",
    "    'num_leaves': 34,\n",
    "    'lambda_l1': 2.407605586973264e-05,\n",
    "    'lambda_l2': 0.005280242664440079,\n",
    "    'metric': 'huber'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    xgb_params = {\n",
    "        'n_estimators': 5000,\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        \"max_bin\": trial.suggest_int('max_bin', 128, 512),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-4, 1.0, log=True),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 2, 4),\n",
    "        \"learning_rate\": trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "        \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.2, 1),\n",
    "        \"colsample_bylevel\": trial.suggest_float('colsample_bylevel', 0.2, 1),\n",
    "        \"colsample_bynode\": trial.suggest_float('colsample_bynode', 0.2, 1),\n",
    "        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"reg:quantileerror\", \"reg:squaredlogerror\", \"reg:squarederror\"]),\n",
    "        #\"tree_method\": \"gpu_hist\",\n",
    "        \"early_stopping_rounds\": 100,\n",
    "        \"random_state\": seed,\n",
    "        \"eval_metric\": \"rmsle\",\n",
    "        \"verbosity\": 0,\n",
    "        \"device\": \"cpu\"\n",
    "    }\n",
    "\n",
    "    if xgb_params[\"objective\"] == \"reg:quantileerror\":\n",
    "        xgb_params[\"quantile_alpha\"] = trial.suggest_float('quantile_alpha', 0.1, 1.0, log=True)\n",
    "\n",
    "    score = []\n",
    "    rkf = RepeatedKFold(n_splits=4, n_repeats=1, random_state=seed)\n",
    "    for i, (tr, val) in tqdm(enumerate(rkf.split(X, y)), total=4):\n",
    "        X_train, X_test, y_train, y_test = X.iloc[tr, :], X.iloc[val, :], y.iloc[tr], y.iloc[val]\n",
    "\n",
    "        xgbmodel = XGBRegressor(**xgb_params)\n",
    "        xgbmodel.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        msle = mean_squared_log_error(y_test, xgbmodel.predict(X_test))\n",
    "        rmsle = np.sqrt(msle)\n",
    "        score.append(rmsle)\n",
    "\n",
    "    print(f\" > RMSLE of XGB =\", score, file=sys.stderr)\n",
    "    return np.mean(score)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, timeout=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators' : 8000,\n",
    "    'max_depth': 4,\n",
    "    'max_bin': 371,\n",
    "    'subsample': 0.44067836416916173,\n",
    "    'alpha': 0.0006151068205422344,\n",
    "    'gamma': 0.0003007261920999099,\n",
    "    'lambda': 2.640618902919003,\n",
    "    'min_child_weight': 3.672380942972272,\n",
    "    'learning_rate': 0.014120863675708872,\n",
    "    'colsample_bytree': 0.8969088050616272,\n",
    "    'colsample_bylevel': 0.5646491700100473,\n",
    "    'colsample_bynode': 0.21803093929408546,\n",
    "    'grow_policy': 'depthwise',\n",
    "    'objective': 'reg:squarederror',\n",
    "    \"tree_method\" : \"gpu_hist\",\n",
    "    \"early_stopping_rounds\" : 1000,\n",
    "    \"random_state\" : seed,\n",
    "    \"eval_metric\": \"rmsle\",\n",
    "    \"verbosity\" :  0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Trial 6 finished with value: 0.02058957683499799 and parameters: {'max_depth': 4, 'max_bin': 371, 'subsample': 0.44067836416916173, 'alpha': 0.0006151068205422344, 'gamma': 0.0003007261920999099, 'lambda': 2.640618902919003, 'min_child_weight': 3.672380942972272, 'learning_rate': 0.014120863675708872, 'colsample_bytree': 0.8969088050616272, 'colsample_bylevel': 0.5646491700100473, 'colsample_bynode': 0.21803093929408546, 'grow_policy': 'depthwise', 'objective': 'reg:squarederror'}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I 2024-08-27 14:04:47,042] Trial 31 finished with value: 0.02058445996801202 and parameters: {'max_depth': 5, 'max_bin': 313, 'subsample': 0.6508269563647683, 'alpha': 7.351826666415423e-08, 'gamma': 0.900513980759424, 'lambda': 2.0746562001247155, 'min_child_weight': 3.7800724402378445, 'learning_rate': 0.011107654821623757, 'colsample_bytree': 0.9277848447908962, 'colsample_bylevel': 0.9125515253916135, 'colsample_bynode': 0.40599231187525964, 'grow_policy': 'lossguide', 'objective': 'reg:squarederror'}. Best is trial 31 with value: 0.02058445996801202.\n",
    "100%|██████████| 4/4 [00:08<00:00,  2.19s/it]\n",
    " > RMSLE of XGB = [0.01854416913947817, 0.023020430986385277, 0.023611248547836135, 0.018377042499863803]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-06 21:17:42,937] A new study created in memory with name: no-name-82a427e9-b295-4856-b309-973cf632345e\n",
      "  0%|          | 0/5 [00:19<?, ?it/s]\n",
      "[W 2024-09-06 21:18:02,507] Trial 0 failed with parameters: {'depth': 11, 'max_bin': 244, 'l2_leaf_reg': 3.172537495334227, 'min_data_in_leaf': 68, 'random_strength': 4.681983482122428, 'learning_rate': 0.03518971818134924, 'max_leaves': 191, 'eval_metric': 'Quantile', 'loss_function': 'RMSE', 'bootstrap_type': 'Poisson'} because of the following error: KeyboardInterrupt('').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1369/3053619220.py\", line 38, in objective\n",
      "    catmodel.fit(train_dataset, use_best_model=True, eval_set=eval_dataset)\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5827, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2400, in _fit\n",
      "    self._train(\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 1780, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4833, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4882, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "[W 2024-09-06 21:18:02,507] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(score)\n\u001b[1;32m     47\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m---> 48\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[1;32m     63\u001b[0m             study,\n\u001b[1;32m     64\u001b[0m             func,\n\u001b[1;32m     65\u001b[0m             n_trials,\n\u001b[1;32m     66\u001b[0m             timeout,\n\u001b[1;32m     67\u001b[0m             catch,\n\u001b[1;32m     68\u001b[0m             callbacks,\n\u001b[1;32m     69\u001b[0m             gc_after_trial,\n\u001b[1;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[13], line 38\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     35\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m Pool(data\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39miloc[val,:], label\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39miloc[val])\n\u001b[1;32m     37\u001b[0m catmodel \u001b[38;5;241m=\u001b[39m CatBoostRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcat_params)\n\u001b[0;32m---> 38\u001b[0m catmodel\u001b[38;5;241m.\u001b[39mfit(train_dataset, use_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, eval_set\u001b[38;5;241m=\u001b[39meval_dataset)\n\u001b[1;32m     40\u001b[0m msle \u001b[38;5;241m=\u001b[39m mean_squared_log_error(y\u001b[38;5;241m.\u001b[39miloc[val], catmodel\u001b[38;5;241m.\u001b[39mpredict(X\u001b[38;5;241m.\u001b[39miloc[val,:]))\n\u001b[1;32m     41\u001b[0m rmsle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(msle)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/catboost/core.py:5827\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   5825\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m-> 5827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline,\n\u001b[1;32m   5828\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[1;32m   5829\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[1;32m   5830\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/catboost/core.py:2400\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2397\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[1;32m   2401\u001b[0m         train_pool,\n\u001b[1;32m   2402\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_sets\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   2403\u001b[0m         params,\n\u001b[1;32m   2404\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2405\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2406\u001b[0m     )\n\u001b[1;32m   2408\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2409\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/catboost/core.py:1780\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[38;5;241m.\u001b[39m_object \u001b[38;5;28;01mif\u001b[39;00m init_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4833\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4882\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from catboost import Pool, CatBoostRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import optuna\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "def objective(trial):\n",
    "    cat_params = {\n",
    "        \"iterations\": 3000,\n",
    "        \"verbose\": False,\n",
    "        'depth': trial.suggest_int('depth', 6, 12), \n",
    "        'max_bin': trial.suggest_int(\"max_bin\", 20, 256), \n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.5, 8.0), \n",
    "        \"min_data_in_leaf\": trial.suggest_int('min_data_in_leaf', 1, 100),         \n",
    "        'random_strength': trial.suggest_float('random_strength', 0.5, 5.0), \n",
    "        \"learning_rate\": trial.suggest_float('learning_rate', 1e-2, 0.2, log=True), \n",
    "        \"max_leaves\": trial.suggest_int('max_leaves', 8, 256), \n",
    "        \"eval_metric\": trial.suggest_categorical(\"eval_metric\",[\"RMSE\",\"Quantile\",\"MSLE\"]),\n",
    "        \"loss_function\": trial.suggest_categorical(\"loss_function\",[\"RMSE\",\"Quantile\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bernoulli\", \"Poisson\"]),\n",
    "        \"task_type\": \"GPU\",\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"random_state\": seed,\n",
    "        \"early_stopping_rounds\": 30,\n",
    "        \"grow_policy\": 'Lossguide' \n",
    "    }\n",
    "        \n",
    "    score = []\n",
    "    for i, (tr, val) in tqdm(enumerate(RepeatedKFold(n_splits=5, n_repeats=1, random_state=seed).split(X, y)), total=5):\n",
    "        X_train, X_test, y_train, y_test = X.iloc[tr,:], X.iloc[val,:], y.iloc[tr], y.iloc[val]\n",
    "        \n",
    "        train_dataset = Pool(data=X.iloc[tr,:], label=y.iloc[tr])\n",
    "        eval_dataset = Pool(data=X.iloc[val,:], label=y.iloc[val])\n",
    "    \n",
    "        catmodel = CatBoostRegressor(**cat_params)\n",
    "        catmodel.fit(train_dataset, use_best_model=True, eval_set=eval_dataset)\n",
    "        \n",
    "        msle = mean_squared_log_error(y.iloc[val], catmodel.predict(X.iloc[val,:]))\n",
    "        rmsle = np.sqrt(msle)\n",
    "        score.append(rmsle)\n",
    "\n",
    "    print(f\" > RMSLE of CAT =\", score, file=sys.stderr)\n",
    "    return np.mean(score)\n",
    "    \n",
    "study = optuna.create_study(direction='minimize') \n",
    "study.optimize(objective, n_trials=100, timeout=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {\n",
    "    \"iterations\": 8000,\n",
    "    \"verbose\": False,\n",
    "    'depth': 6,\n",
    "    'max_bin': 92,\n",
    "    'l2_leaf_reg': 3.5831191004106717,\n",
    "    'min_data_in_leaf': 37,\n",
    "    'random_strength': 1.2543270199229692,\n",
    "    'learning_rate': 0.035712453455385845,\n",
    "    'max_leaves': 118,\n",
    "    'eval_metric': 'RMSE',\n",
    "    'loss_function': 'RMSE',\n",
    "    'bootstrap_type': 'Poisson',\n",
    "    \"grow_policy\": 'Lossguide',\n",
    "    \"task_type\": \"GPU\",\n",
    "    \"random_state\": seed,\n",
    "    \"early_stopping_rounds\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 89 finished with value: 0.020468911093357866 and parameters: {'depth': 6, 'max_bin': 92, 'l2_leaf_reg': 3.5831191004106717, 'min_data_in_leaf': 37, 'random_strength': 1.2543270199229692, 'learning_rate': 0.035712453455385845, 'max_leaves': 118, 'eval_metric': 'MSLE', 'loss_function': 'RMSE', 'bootstrap_type': 'Poisson'}. Best is trial 89 with value: 0.020468911093357866."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-10 21:19:33,126] A new study created in memory with name: no-name-4e97fb55-b7b5-40ab-a460-a35d623efc3a\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:31<01:35, 31.74s/it]\n",
      " 50%|█████     | 2/4 [01:02<01:02, 31.29s/it]\n",
      " 75%|███████▌  | 3/4 [01:31<00:30, 30.04s/it]\n",
      "100%|██████████| 4/4 [01:58<00:00, 29.72s/it]\n",
      "[I 2024-09-10 21:21:32,013] Trial 0 finished with value: 0.02084475751062633 and parameters: {'xgb_wt': 8.014485973509426, 'lgbm_wt': 2.5444905238673243, 'cat_wt': 1.152603522489306}. Best is trial 0 with value: 0.02084475751062633.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:27<01:21, 27.02s/it]\n",
      " 50%|█████     | 2/4 [00:53<00:52, 26.44s/it]\n",
      " 75%|███████▌  | 3/4 [01:27<00:30, 30.03s/it]\n",
      "100%|██████████| 4/4 [01:53<00:00, 28.34s/it]\n",
      "[I 2024-09-10 21:23:25,369] Trial 1 finished with value: 0.020821467379592763 and parameters: {'xgb_wt': 4.6094788125823865, 'lgbm_wt': 0.2314903485932751, 'cat_wt': 4.325269006042918}. Best is trial 1 with value: 0.020821467379592763.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:27<01:22, 27.66s/it]\n",
      " 50%|█████     | 2/4 [00:58<00:59, 29.57s/it]\n",
      " 75%|███████▌  | 3/4 [01:27<00:29, 29.45s/it]\n",
      "100%|██████████| 4/4 [01:54<00:00, 28.66s/it]\n",
      "[I 2024-09-10 21:25:19,997] Trial 2 finished with value: 0.020860239382839006 and parameters: {'xgb_wt': 9.532828582917698, 'lgbm_wt': 6.752006250108511, 'cat_wt': 7.272317980713101}. Best is trial 1 with value: 0.020821467379592763.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:26<01:19, 26.48s/it]\n",
      " 50%|█████     | 2/4 [00:53<00:53, 26.54s/it]\n",
      " 75%|███████▌  | 3/4 [01:23<00:28, 28.43s/it]\n",
      "100%|██████████| 4/4 [01:52<00:00, 28.24s/it]\n",
      "[I 2024-09-10 21:27:12,953] Trial 3 finished with value: 0.020820227512628756 and parameters: {'xgb_wt': 3.272388738328673, 'lgbm_wt': 0.1312984345987378, 'cat_wt': 3.2969672606343092}. Best is trial 3 with value: 0.020820227512628756.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:27<01:23, 27.84s/it]\n",
      " 50%|█████     | 2/4 [00:55<00:54, 27.46s/it]\n",
      " 75%|███████▌  | 3/4 [01:24<00:28, 28.15s/it]\n",
      "100%|██████████| 4/4 [01:50<00:00, 27.60s/it]\n",
      "[I 2024-09-10 21:29:03,355] Trial 4 finished with value: 0.020607151354076994 and parameters: {'xgb_wt': 9.945475965974918, 'lgbm_wt': 6.800000111862375, 'cat_wt': 8.903716908538927}. Best is trial 4 with value: 0.020607151354076994.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:26<01:20, 26.95s/it]\n",
      " 50%|█████     | 2/4 [00:58<00:59, 29.55s/it]\n",
      " 75%|███████▌  | 3/4 [01:28<00:30, 30.06s/it]\n",
      "100%|██████████| 4/4 [01:55<00:00, 28.84s/it]\n",
      "[I 2024-09-10 21:30:58,730] Trial 5 finished with value: 0.020583862346664556 and parameters: {'xgb_wt': 5.142907615056636, 'lgbm_wt': 1.945580988860961, 'cat_wt': 6.971739300852625}. Best is trial 5 with value: 0.020583862346664556.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:26<01:18, 26.29s/it]\n",
      " 50%|█████     | 2/4 [00:55<00:56, 28.05s/it]\n",
      " 75%|███████▌  | 3/4 [01:25<00:29, 29.11s/it]\n",
      "100%|██████████| 4/4 [01:54<00:00, 28.57s/it]\n",
      "[I 2024-09-10 21:32:53,014] Trial 6 finished with value: 0.020577328947503054 and parameters: {'xgb_wt': 4.637829198773009, 'lgbm_wt': 8.576036966614105, 'cat_wt': 0.35648175292021134}. Best is trial 6 with value: 0.020577328947503054.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:34<01:43, 34.57s/it]\n",
      " 50%|█████     | 2/4 [01:02<01:01, 30.94s/it]\n",
      " 75%|███████▌  | 3/4 [01:35<00:31, 31.83s/it]\n",
      "100%|██████████| 4/4 [02:02<00:00, 30.74s/it]\n",
      "[I 2024-09-10 21:34:55,973] Trial 7 finished with value: 0.020802621502718513 and parameters: {'xgb_wt': 7.609742961842212, 'lgbm_wt': 1.5413795607103764, 'cat_wt': 1.8518606915930624}. Best is trial 6 with value: 0.020577328947503054.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:25<01:16, 25.62s/it]\n",
      " 50%|█████     | 2/4 [00:53<00:54, 27.18s/it]\n",
      " 75%|███████▌  | 3/4 [01:20<00:27, 27.06s/it]\n",
      "100%|██████████| 4/4 [01:47<00:00, 26.98s/it]\n",
      "[I 2024-09-10 21:36:43,905] Trial 8 finished with value: 0.02086819071181647 and parameters: {'xgb_wt': 3.98126245637499, 'lgbm_wt': 2.464793313755671, 'cat_wt': 7.510743279381112}. Best is trial 6 with value: 0.020577328947503054.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:29<01:27, 29.19s/it]\n",
      " 50%|█████     | 2/4 [00:56<00:56, 28.34s/it]\n",
      " 75%|███████▌  | 3/4 [01:26<00:28, 28.92s/it]\n",
      "100%|██████████| 4/4 [01:54<00:00, 28.56s/it]\n",
      "[I 2024-09-10 21:38:38,160] Trial 9 finished with value: 0.02087372060808148 and parameters: {'xgb_wt': 6.93242639315593, 'lgbm_wt': 9.978220197614192, 'cat_wt': 9.071151600659977}. Best is trial 6 with value: 0.020577328947503054.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:25<01:17, 25.98s/it]\n",
      " 50%|█████     | 2/4 [00:51<00:51, 25.64s/it]\n",
      " 75%|███████▌  | 3/4 [01:17<00:25, 25.88s/it]\n",
      "100%|██████████| 4/4 [01:46<00:00, 26.53s/it]\n",
      "[I 2024-09-10 21:40:24,282] Trial 10 finished with value: 0.0208514562053584 and parameters: {'xgb_wt': 1.3043643079897849, 'lgbm_wt': 9.49854489626919, 'cat_wt': 0.12788687533177945}. Best is trial 6 with value: 0.020577328947503054.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:29<01:28, 29.51s/it]\n",
      " 50%|█████     | 2/4 [00:58<00:58, 29.11s/it]\n",
      " 75%|███████▌  | 3/4 [01:25<00:28, 28.31s/it]\n",
      "100%|██████████| 4/4 [01:55<00:00, 28.79s/it]\n",
      "[I 2024-09-10 21:42:19,455] Trial 11 finished with value: 0.020846541460699506 and parameters: {'xgb_wt': 5.734727374853242, 'lgbm_wt': 4.620174610631602, 'cat_wt': 5.852326685487122}. Best is trial 6 with value: 0.020577328947503054.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:26<01:18, 26.10s/it]\n",
      " 50%|█████     | 2/4 [00:53<00:53, 26.72s/it]\n",
      " 75%|███████▌  | 3/4 [01:24<00:28, 28.60s/it]\n",
      "100%|██████████| 4/4 [01:51<00:00, 27.86s/it]\n",
      "[I 2024-09-10 21:44:10,908] Trial 12 finished with value: 0.020860846121302082 and parameters: {'xgb_wt': 1.9259072029058188, 'lgbm_wt': 5.077405999342315, 'cat_wt': 5.8244026954585575}. Best is trial 6 with value: 0.020577328947503054.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:31<01:33, 31.21s/it]\n",
      " 50%|█████     | 2/4 [00:58<00:57, 28.83s/it]\n",
      " 75%|███████▌  | 3/4 [01:24<00:27, 27.60s/it]\n",
      "100%|██████████| 4/4 [01:56<00:00, 29.24s/it]\n",
      "[I 2024-09-10 21:46:07,867] Trial 13 finished with value: 0.02086197846175434 and parameters: {'xgb_wt': 5.663798715238951, 'lgbm_wt': 8.053905510013898, 'cat_wt': 3.195036097861257}. Best is trial 6 with value: 0.020577328947503054.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:30<01:31, 30.51s/it]\n",
      " 50%|█████     | 2/4 [00:56<00:55, 27.87s/it]\n",
      " 75%|███████▌  | 3/4 [01:27<00:29, 29.47s/it]\n",
      "100%|██████████| 4/4 [02:03<00:00, 30.89s/it]\n",
      "[I 2024-09-10 21:48:11,441] Trial 14 finished with value: 0.020852580138777603 and parameters: {'xgb_wt': 2.967251178965002, 'lgbm_wt': 4.613945860634865, 'cat_wt': 7.179297384408331}. Best is trial 6 with value: 0.020577328947503054.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:26<01:20, 26.91s/it]\n",
      " 50%|█████     | 2/4 [00:55<00:55, 27.90s/it]\n",
      " 75%|███████▌  | 3/4 [01:23<00:27, 27.76s/it]\n",
      "100%|██████████| 4/4 [01:51<00:00, 27.88s/it]\n",
      "[I 2024-09-10 21:50:02,965] Trial 15 finished with value: 0.020564163217832637 and parameters: {'xgb_wt': 6.3064000713942345, 'lgbm_wt': 3.7262620506503454, 'cat_wt': 4.99519563732956}. Best is trial 15 with value: 0.020564163217832637.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:26<01:20, 26.88s/it]\n",
      " 50%|█████     | 2/4 [00:54<00:54, 27.07s/it]\n",
      " 75%|███████▌  | 3/4 [01:20<00:26, 26.84s/it]\n",
      "100%|██████████| 4/4 [01:49<00:00, 27.25s/it]\n",
      "[I 2024-09-10 21:51:51,988] Trial 16 finished with value: 0.020828462440141277 and parameters: {'xgb_wt': 7.087174868987453, 'lgbm_wt': 3.5455535166072365, 'cat_wt': 2.257371372514386}. Best is trial 15 with value: 0.020564163217832637.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:28<01:25, 28.36s/it]\n",
      " 50%|█████     | 2/4 [00:56<00:56, 28.36s/it]\n",
      " 75%|███████▌  | 3/4 [01:25<00:28, 28.77s/it]\n",
      "100%|██████████| 4/4 [01:55<00:00, 28.79s/it]\n",
      "[I 2024-09-10 21:53:47,149] Trial 17 finished with value: 0.020839929946723682 and parameters: {'xgb_wt': 0.4668225173487368, 'lgbm_wt': 6.0472361278570546, 'cat_wt': 4.429127924849619}. Best is trial 15 with value: 0.020564163217832637.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:26<01:18, 26.24s/it]\n",
      " 50%|█████     | 2/4 [00:54<00:54, 27.30s/it]\n",
      " 75%|███████▌  | 3/4 [01:23<00:28, 28.02s/it]\n",
      "100%|██████████| 4/4 [01:50<00:00, 27.71s/it]\n",
      "[I 2024-09-10 21:55:38,019] Trial 18 finished with value: 0.020839429830925252 and parameters: {'xgb_wt': 6.084391975004792, 'lgbm_wt': 8.404177585632906, 'cat_wt': 0.0971187650120946}. Best is trial 15 with value: 0.020564163217832637.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:26<01:18, 26.23s/it]\n",
      " 50%|█████     | 2/4 [00:53<00:53, 26.99s/it]\n",
      " 75%|███████▌  | 3/4 [01:22<00:27, 27.83s/it]\n",
      "100%|██████████| 4/4 [01:49<00:00, 27.37s/it]\n",
      "[I 2024-09-10 21:57:27,499] Trial 19 finished with value: 0.020597281977848762 and parameters: {'xgb_wt': 8.32195518009526, 'lgbm_wt': 3.4550093703243023, 'cat_wt': 9.85957478948438}. Best is trial 15 with value: 0.020564163217832637.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:27<01:22, 27.47s/it]\n",
      " 25%|██▌       | 1/4 [00:54<02:44, 54.82s/it]\n",
      "[W 2024-09-10 21:58:22,321] Trial 20 failed with parameters: {'xgb_wt': 3.528379409267524, 'lgbm_wt': 8.182709851360046, 'cat_wt': 3.3337516945333645} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1467/2976117896.py\", line 24, in objective\n",
      "    xgbmodel.fit(X_train,y_train, eval_set=[(X_test,y_test)],verbose = 0)\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/ubuntu01/anaconda3/lib/python3.11/site-packages/xgboost/core.py\", line 2101, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-09-10 21:58:22,324] Trial 20 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(RMSLE)\n\u001b[1;32m     36\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[1;32m     63\u001b[0m             study,\n\u001b[1;32m     64\u001b[0m             func,\n\u001b[1;32m     65\u001b[0m             n_trials,\n\u001b[1;32m     66\u001b[0m             timeout,\n\u001b[1;32m     67\u001b[0m             catch,\n\u001b[1;32m     68\u001b[0m             callbacks,\n\u001b[1;32m     69\u001b[0m             gc_after_trial,\n\u001b[1;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGB_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, file \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m     23\u001b[0m xgbmodel \u001b[38;5;241m=\u001b[39m XGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mxgb_params)\n\u001b[0;32m---> 24\u001b[0m xgbmodel\u001b[38;5;241m.\u001b[39mfit(X_train,y_train, eval_set\u001b[38;5;241m=\u001b[39m[(X_test,y_test)],verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m xgb_preds \u001b[38;5;241m=\u001b[39m xgbmodel\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     27\u001b[0m lgbm_preds \u001b[38;5;241m=\u001b[39m lgbmmodel\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1109\u001b[0m     params,\n\u001b[1;32m   1110\u001b[0m     train_dmatrix,\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1112\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1113\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1114\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1115\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1116\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1117\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1118\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1119\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2103\u001b[0m         )\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    xgb_wt =  trial.suggest_float('xgb_wt',0,10)\n",
    "    lgbm_wt = trial.suggest_float('lgbm_wt',0,10)\n",
    "    cat_wt = trial.suggest_float('cat_wt',0,10)\n",
    "    RMSLE = []\n",
    "\n",
    "    for i,(tr,val) in tqdm(enumerate(RepeatedKFold(n_splits=4, n_repeats=1,random_state=seed).split(X,y)),total = 4):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = X.iloc[tr,:],X.iloc[val,:],y.iloc[tr],y.iloc[val]\n",
    "        \n",
    "        print(f\"\\nLGBM_{i+1}\",end=\" | \", file = sys.stderr)\n",
    "        lgbmmodel = LGBMRegressor(**lgbm_params)\n",
    "        lgbmmodel.fit(X_train,y_train, eval_set=[(X_test,y_test)], eval_names=[\"valid\"],eval_metric=['MSLE'])\n",
    "\n",
    "        print(f\"CAT_{i+1}\",end=\" | \", file = sys.stderr)\n",
    "        train_dataset = Pool(data=X.iloc[tr,:],label=y.iloc[tr])\n",
    "        eval_dataset = Pool(data=X.iloc[val,:],label=y.iloc[val])\n",
    "        catmodel = CatBoostRegressor(**cat_params)\n",
    "        catmodel.fit(train_dataset, use_best_model=True, eval_set=eval_dataset)\n",
    "\n",
    "        print(f\"XGB_{i+1}\", end = \"\", file = sys.stderr)\n",
    "        xgbmodel = XGBRegressor(**xgb_params)\n",
    "        xgbmodel.fit(X_train,y_train, eval_set=[(X_test,y_test)],verbose = 0)\n",
    "\n",
    "        xgb_preds = xgbmodel.predict(X_test)\n",
    "        lgbm_preds = lgbmmodel.predict(X_test)\n",
    "        cat_preds = catmodel.predict(X_test)\n",
    "\n",
    "        preds = ((xgb_wt*xgb_preds)+(lgbm_wt*lgbm_preds)+(cat_wt*cat_preds))/(xgb_wt+cat_wt+lgbm_wt)\n",
    "        msle = mean_squared_log_error(y_test, preds)\n",
    "\n",
    "        RMSLE.append(np.sqrt(msle))\n",
    "    return np.mean(RMSLE)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=200,timeout=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
